{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9aNa+pBBtRN1jpoSd9YAe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhawana1009/Regression-Analysis/blob/main/Regression_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRvXw-be1wmC"
      },
      "outputs": [],
      "source": [
        "# Econometrics and Data Analysis for Accounting and Finance (MN52080)\n",
        "# Empirical Project 1 â€“ Individual Work\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.stats.api as sms\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.compat import lzip\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from statsmodels.stats.diagnostic import het_white\n",
        "from statsmodels.stats.outliers_influence import reset_ramsey\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "data = pd.read_excel(\"C:\\\\econonometrics\\\\econometrics\\\\Goyal_Welch_data.xlsx\" , index_col=0)\n",
        "print(data.head())\n",
        "\n",
        "data = data.loc['1970-01':'2020-12']\n",
        "print(data)\n",
        "\n",
        "data.index = pd.to_datetime(data.index ,format='%Y%m')\n",
        "data = data.fillna(method='ffill')\n",
        "print(data.head())\n",
        "\n",
        "# DATAFRAME\n",
        "data = pd.DataFrame({\n",
        "    'Equity_Premium':np.log(data['CRSP_SPvw']) -np.log(data['Rfree']),\n",
        "    'tms': data['lty'] - data['tbl'],\n",
        "    'dfy': data['BAA'] - data['AAA'],\n",
        "    'dfr': data['corpr']-data['ltr'],\n",
        "    'tbl': data['tbl'],\n",
        "    'ltr': data['ltr'],\n",
        "    'Infl': data['infl']\n",
        "\n",
        "})\n",
        "print(data.head())\n",
        "\n",
        "# MULTIPLE REGRESSION\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "formula = 'Equity_Premium ~ ltr + tbl + tms  + dfr + dfy + Infl'\n",
        "results = smf.ols(formula, data=data).fit()\n",
        "print(results.summary())\n",
        "\n",
        "\n",
        "# F Test\n",
        "hypotheses = ' tms= ltr = tbl = dfr = dfy = Infl = 0'\n",
        "f_test = results.f_test(hypotheses)\n",
        "print(f_test)\n",
        "\n",
        "# NORMAL DISTRIBUTION\n",
        "# JARQUE BERA\n",
        "residuals = results.resid\n",
        "name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']\n",
        "test = sms.jarque_bera(residuals)\n",
        "lzip(name, test)\n",
        "\n",
        "# RESET RAMSEY\n",
        "formula = 'Equity_Premium ~ tms + ltr + tbl  + dfy + dfr + Infl'\n",
        "results = smf.ols(formula, data).fit()\n",
        "reset_ramsey(results,degree = 4)\n",
        "\n",
        "residuals = results.resid\n",
        "plt.figure(1, dpi=100)\n",
        "plt.hist(residuals,20,edgecolor='black',linewidth=1.2)\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Density')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# AUTOCORRELATION\n",
        "with open('macro.pickle', 'rb') as handle:\n",
        " loaded_data = pickle.load(handle)\n",
        "\n",
        "loaded_data = loaded_data.dropna()\n",
        "\n",
        "# DURBIN WASTON TEST\n",
        "residuals = results.resid\n",
        "durbin_watson_stat = sms.durbin_watson(residuals)\n",
        "print(durbin_watson_stat)\n",
        "\n",
        "# BREUSCH GODFREY TEST\n",
        "name = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\n",
        "results1 = sms.acorr_breusch_godfrey(results, 12)\n",
        "lzip(name, results1)\n",
        "\n",
        "data = data.dropna()\n",
        "data.index = pd.to_datetime(data.index)\n",
        "formula = 'Equity_Premium ~ tms  + tbl  + dfy  + dfr + ltr + Infl'\n",
        "results = smf.ols(formula, data).fit()\n",
        "\n",
        "# OBTAIN THE RESIDUALS AND PLOT OUT RESIDUALS OVER TIME\n",
        "plt.figure(1)\n",
        "plt.plot(results.resid)\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Residuals')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# BREUSCH PAGAN\n",
        "name = ['Lagrange multiplier statistic', 'Lagrange multiplier p-value', 'F-statistic', 'F-statistic p-value']\n",
        "breuschpagan_test = sms.het_breuschpagan(results.resid, results.model.exog)\n",
        "lzip(name, breuschpagan_test)\n",
        "\n",
        "# WHITE TEST\n",
        "lm, lm_pvalue, fvalue, f_pvalue = het_white(results.resid, results.model.exog)\n",
        "print(\"Lagrange multiplier statistic:\",lm)\n",
        "print(\"Lagrange multiplier p-value:\",lm_pvalue)\n",
        "print(\"F-statistic:\",fvalue)\n",
        "print(\"F-statistic p-value:\",f_pvalue)\n",
        "if lm_pvalue < 0.05:\n",
        " print(\"Rejected the null hypothesis. Heteroscedasticity detected\")\n",
        "else:\n",
        " print(\"Failed to rejected the null hypothesis. No heteroscedasticity detected\")\n",
        "\n",
        " # GOLDFELD QUANDT TEST\n",
        "y = data['Equity_Premium']\n",
        "explanatory_var = data[['tms','tbl','ltr','dfy','dfr', 'Infl']]\n",
        "x = sm.add_constant(explanatory_var)\n",
        "sm.stats.diagnostic.het_goldfeldquandt(y, x, drop=0.25)\n",
        "\n",
        "# WHITE MODIFIED STANDARD ERROR\n",
        "results_White = smf.ols(formula, data).fit(cov_type='HC1')\n",
        "print(results_White.summary())\n",
        "\n",
        "# NEWEY WEST PROCEDURE\n",
        "results_NW = smf.ols(formula, data).fit(cov_type='HAC',\n",
        " cov_kwds={'maxlags':6,'use_correction':True})\n",
        "print(results_NW.summary())\n",
        "\n",
        "# CORRELATION MATRIX AND VIF\n",
        "explanatory_var.corr()\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = explanatory_var.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(explanatory_var.values, i)\n",
        " for i in range(len(explanatory_var.columns))]\n",
        "\n",
        "print(vif_data)\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(data.corr())\n",
        "\n",
        "formula_1 ='Equity_Premium ~ tms + dfr  + tbl  + dfy + ltr + Infl '\n",
        "results_1 = smf.ols(formula_1, data).fit()\n",
        "y_fitted = results_1.fittedvalues\n",
        "residuals = results_1.resid\n",
        "\n",
        "plt.figure(1,dpi=100)\n",
        "plt.plot(residuals, label='resid')\n",
        "plt.plot(y_fitted, label='linear prediction')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Residuals')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "residuals.nsmallest(2)\n",
        "\n",
        "# DUMMY VARIABLE\n",
        "data['JANDUM'] = np.where(data.index.month == 1, 1, 0)\n",
        "data['DEC00DUM'] = np.where(data.index == '1973-11-1', 1, 0)\n",
        "data['JUN00DUM'] = np.where(data.index == '1975-3-1', 1, 0)\n",
        "formula_2 = 'Equity_Premium ~ tms + ltr + tbl  + dfy + dfr + Infl + DEC00DUM + JUN00DUM + JANDUM'\n",
        "results_2 = smf.ols(formula_2, data).fit()\n",
        "print(results_2.summary())\n",
        "\n",
        "\n",
        "# T TEST\n",
        "from scipy.stats import t\n",
        "\n",
        "df = 334\n",
        "\n",
        "alpha_1 = 0.01\n",
        "alpha_5 = 0.05\n",
        "alpha_10 = 0.1\n",
        "\n",
        "t_critical_1 = t.ppf(1 - alpha_1/2, df)\n",
        "t_critical_5 = t.ppf(1 - alpha_5/2, df)\n",
        "t_critical_10 = t.ppf(1 - alpha_10/2, df)\n",
        "print(f\"1% Critical Value: {t_critical_1:.4f}\")\n",
        "print(f\"5% Critical Value: {t_critical_5:.4f}\")\n",
        "print(f\"10% Critical Value: {t_critical_10:.4f}\")\n",
        "\n",
        "\n"
      ]
    }
  ]
}